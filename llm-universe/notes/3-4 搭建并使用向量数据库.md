# 3-4 搭建并使用向量数据库

### 1. 前序配置

```python
# -*- coding: utf-8 -*-
# @Time    : 2024/8/27/027 17:45
# @Author  : Shining
# @File    : dataset.py
# @Description :


import os
from dotenv import find_dotenv,load_dotenv
from langchain.document_loaders.pdf import PyMuPDFLoader
from langchain.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re

# 配置环境变量
_ = load_dotenv(find_dotenv())
# 读取需要加入数据库的文件 这里只有一个 直接给了
file_paths = ["中华人民共和国民法典.pdf"]

# 创建数据加载器
def create_loader(file_paths):
    loaders = []
    for file_path in file_paths:
        if file_path.endswith("pdf"):
            loaders.append(PyMuPDFLoader(file_path))
        elif file_path.endswith("md"):
            loaders.append(UnstructuredMarkdownLoader(file_path))
    return loaders

# 加载数据
def load_text(loaders):
    texts = []
    for loader in loaders:
        texts.extend(loader.load())
    return texts

# 清洗数据
def wash_data(docs):
    # for i in range(len(docs)):
    pdf_page = docs
    """
    查看原本数据\n的分布情况:
        1.每一页pdf在读取时，在开头存在页码
        2.保留第一节 第一条 第字前面的换行符
    """
    pattern = re.compile(r'[$\d]', re.DOTALL)
    pdf_page.page_content = re.sub(pattern, "", pdf_page.page_content)
    pattern = re.compile(r'[$\d]', re.DOTALL)
    pdf_page.page_content = re.sub(pattern, "", pdf_page.page_content)
    pattern = re.compile(r'[$\d]', re.DOTALL)
    pdf_page.page_content = re.sub(pattern, "", pdf_page.page_content)
    pattern = re.compile(r'(\n)[$\u4e00-\u7b2b,\u7b2d-\u9fff]', re.DOTALL)
    pdf_page.page_content = re.sub(pattern, "", pdf_page.page_content)



if __name__ == '__main__':
    loaders = create_loader(file_paths)
    # langchain_core.documents.base.Document类型
    texts = load_text(loaders)
    # 清洗数据
    for doc in texts:
        wash_data(doc)
    # 分割文档
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)
    split_docs = text_splitter.split_documents(texts)
```

### 2. 构建Chroma向量库

	Langchain集成了超过 30 个不同的向量存储库，选择**Chroma**是因为**它轻量级且数据存储在内存中**，这使得它非常容易启动和开始使用。

LangChain使用embedding的方式：

* 使用OpenAI接口
* 使用百度千帆接口
* 针对其不支持的Embedding接口进行自定义封装

```python
# -*- coding: utf-8 -*-
# @Time    : 2024/8/27/027 18:22
# @Author  : Shining
# @File    : zhipu_embedding.py
# @Description :


from langchain.embeddings.base import Embeddings
from typing import List

class ZhipuAIEmbeddings(Embeddings):
    def __init__(self):
        self.client = self._create_client()
    def _create_client(self):
        from zhipuai import ZhipuAI
        return ZhipuAI()

    def embed_query(self, text: str) -> List[float]:
        embeddings = self.client.embeddings.create(
            model="embedding-2",
            input=text
        )
        return embeddings.data[0].embedding

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.embed_query(text) for text in texts]

    from zhipu_embedding import ZhipuAIEmbeddings

    zhipu_embedding = ZhipuAIEmbeddings()
    persist_directory = r'C:\llm_study\lll-universe\codes\chapter-3'

    from langchain.vectorstores.chroma import Chroma

    vectordb = Chroma.from_documents(
        documents=split_docs[:20],  # 为了速度，只选择前 20 个切分的 doc 进行生成；使用千帆时因QPS限制，建议选择前 5 个doc
        embedding=zhipu_embedding,
        persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上
    )

    vectordb.persist()
    print(f"向量库中存储的数量：{vectordb._collection.count()}")
```

### 3. 向量检索

##### 3.1 相似度检索

	Chroma的相似度搜索使用的是**余弦距离，** 当需要严谨的按照余弦相似度排序的结果可以使用similarity_search函数

```python
# -*- coding: utf-8 -*-
# @Time    : 2024/8/27/027 18:44
# @Author  : Shining
# @File    : vector_retrieval.py
# @Description :


from dataset import vectordb

question = "什么是民法典？"

sim_docs = vectordb.similarity_search(question,k=3)
print(sim_docs)
print(len(sim_docs))
```

##### 3.2 MMR检索

	MMR（最大边际相关性）可以帮助检索到的内容**保持相关性**的同时，又能**增加检索到内容的丰富性**。具体来说，就是在选择一个相似度高的文档后，再选择一个相似度较低但是信息丰富的文档。

```python
# -*- coding: utf-8 -*-
# @Time    : 2024/8/27/027 18:44
# @Author  : Shining
# @File    : vector_retrieval.py
# @Description :


from dataset import vectordb

question = "什么是民法典？"

# sim_docs = vectordb.similarity_search(question,k=3)
mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)
print(mmr_docs)
print(len(mmr_docs))
```

##### 附录 embedding类封装讲解

	要实现自定义embedding，需要定义一个自定义类**继承自LangChain的Embeddings基类**，然后定义两个函数：

* embed\_query方法：用于对单个字符串（query）进行embedding
* embed\_documents方法，用于对字符串列表（documents）进行embedding

	导入库

```python
from __future__ import annotations

import logging
from typing import Dict, List, Any

from langchain.embeddings.base import Embeddings
from langchain.pydantic_v1 import BaseModel, root_validator

logger = logging.getLogger(__name__)
```

	继承Embeddings类与BaseModel类的自定义Embeddings类（Pydantic用于数据验证和设置管理。通过类型注解，提供了简单而高效的数据验证机制。核心组件是BaseModel类，通过继承这个类，可以定义具有数据验证和序列化功能的模型）：

```python
class ZhipuAIEmbeddings(BaseModel, Embeddings):
    """`Zhipuai Embeddings` embedding models."""

    client: Any
    """`zhipuai.ZhipuAI"""
```

	root\_validator用于在**校验整个数据模型之前对整个数据模型进行自定义校验**，以确保所有的数据都符合所期望的数据结构。root\_validator 接收一个函数作为参数，该函数包含需要校验的逻辑。函数应该返回一个字典，其中包含经过校验的数据。如果校验失败，则抛出ValueError 异常。

```python
@root_validator()
def validate_environment(cls, values: Dict) -> Dict:
    """
    实例化ZhipuAI为values["client"]

    Args:

        values (Dict): 包含配置信息的字典，必须包含 client 的字段.
    Returns:

        values (Dict): 包含配置信息的字典。如果环境中有zhipuai库，则将返回实例化的ZhipuAI类；否则将报错 'ModuleNotFoundError: No module named 'zhipuai''.
    """
    from zhipuai import ZhipuAI
    values["client"] = ZhipuAI()
    return values
```
