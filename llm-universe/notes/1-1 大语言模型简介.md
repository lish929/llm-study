# 1-1 大语言模型简介

### 1. 什么是大语言模型（LLM）

	**大语言模型是一种旨在理解和生成人类语言的人工智能模型**。通常指包含**数百亿（或更多）参数的语言模型**，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。

#### 1.1 知名大语言模型

国外知名大语言模型：

* GPT-3.5 GPT-4：由 OpenAl 开发，广泛用于各种 NLP 任务，包括生成文本、翻译、问答等。
* PaLM：由 Google 开发，是一个拥有 5400 亿参数的超大规模模型。
* Claude LLaMA：由 Anthropic 和 Meta 开发，分别致力于安全和高效的自然语言理解。

国内知名大语言模型：

* 文心一言：由百度开发，应用于智能搜索和对话系统。
* 讯飞星火：由科大讯飞开发，广泛应用于教育、医疗等领域。
* 通义千问：由阿里巴巴开发，专注于企业级解决方案。
* ChatGLM 百川：也在国内具备一定影响力，应用于多种行业场景。

#### 1.2 LLM发展历程

* 20世纪90年代：采用**统计学习方法**来预测词汇，通过分析前面的词汇来预测下一个词汇。但在理解复杂语言规则方面存在一定局限性。
* 2003年：深度学习先驱 **Bengio** 在论文[A Neural Probabilistic Language Model](https://dl.acm.org/doi/pdf/10.5555/944919.944966)首次将神经网络融入到语言模型
* 2018年：**Transformer架构**出现，通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，极大地提升了模型在各种自然语言处理任务上的表现。
* 随着**语言模型规模的扩大（增加模型大小或使用更多数据）** ，模型展现出了一些惊人的能力，在各种任务中的表现均显著提升。

#### 1.3 大语言模型的“涌现能力”

* BERT：3.3亿参数 GPT-2：15亿参数

* GPT-3：1750亿参数 PaLM：5400亿参数

	上述大语言模型使用**相似的架构和预训练任务**，但它们展现出截然不同的能力。区分大语言模型（LLM）与以前的预训练语言模型（PLM）最显著的特征之一是它们的**涌现能力** 。涌现能力就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的**量变引起质变**。

### 2. LLM能力与特点

##### 2.1 涌现能力

LLM典型的涌现能力有以下几点：

* 上下文学习：由 GPT-3 首次引入的。允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。
* 指令遵循：通过使用自然语言描述的多任务数据进行微调，也就是所谓的**指令微调**
* 逐步推理：LLM 通过采用**思维链**推理策略，利用包含中间推理步骤的提示机制来解决这些任务。

##### 2.2 基座模型

	借助于**海量无标注数据的训练**，获得可以**适用于大量下游任务的大模型**（单模态或者多模态）。**多个应用可以只依赖于一个或少数几个大模型进行统一建设**。大语言模型是这个新模式的典型例子，使用统一的大模型可以极大地提高研发效率。

##### 2.3 支持对话作为统一入口的能力

##### 2.4 LLM特点

* 规模巨大：前面提到大模型可以作为基座模型，使多个应用依赖于一个或烧出几个大模型进行统一的建设，虽然这样提升了开发效率，但是却受困LLM巨大的规模，部署困难
* 预训练与微调：首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务。
* 上下文感知：理解和生成依赖于前文的文本内容
* 多语言支持
* 多模态支持：扩展到支持多模态数据，包括文本、图像和声音
* 伦理和风险问题
* 高计算资源需求

### 3. LLM的应用和影响

* **自然语言处理**领域，它可以帮助计算机更好地理解和生成文本，包括写文章、回答问题、翻译语言等。
* **信息检索**领域，它可以改进搜索引擎，让我们更轻松地找到所需的信息。
* **计算机视觉**领域，研究人员还在努力让计算机理解图像和文字，以改善多媒体交互。
*  **通用人工智能（AGI），** AGI 是一种像人类一样思考和学习的人工智能。LLM 被认为是 AGI 的一种早期形式，这引发了对未来人工智能发展的许多思考和计划。

‍

‍
