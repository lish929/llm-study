# 1-2 检索增强生成RAG简介

### 1. 什么是RAG

	大型语言模型（LLM）相较于传统的语言模型具有更强大的能力，然而在某些情况下，它们仍可能无法提供准确的答案。**检索增强生成（RAG, Retrieval-Augmented Generation）整合了从庞大知识库中检索到的相关信息，并以此为基础，指导大型语言模型生成更为精准的答案**，从而显著提升了回答的准确性与深度。

目前 LLM 面临的主要问题有：

* **信息偏差/幻觉：**  LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。
* **知识更新滞后性：**  LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。
* **内容不可追溯：**  LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。
* **领域专业知识能力欠缺：**  LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。
* **推理能力限制：**  面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。
* **应用场景适应性受限：**  LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。
* **长文本处理能力较弱：**  LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。

### 2. RAG工作流程

* 数据处理阶段

  * 清洗处理原始数据
  * 转换数据为检索模型可用格式
  * 将数据存储进数据库
* 检索阶段

  * 用户输入问题，从数据库检索相关信息
* 增强阶段

  * 对数据检索的信息进行处理，提供给生成模型使用
* 生成阶段

  * LLM根据处理过后的信息，生成针对于用户问题的答案

### 3. RAG 对比 Finetune

|特征比较|RAG|微调|
| ----------| ------------------------------------------------------------------------| ----------------------------------------------------------------------------|
|知识更新|直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。|通常需要重新训练来保持知识和数据的更新。更新成本高，适合静态数据。|
|外部知识|擅长利用外部资源，特别适合处理文档或其他结构化/非结构化数据库。|将外部知识学习到 LLM 内部。|
|数据处理|对数据的处理和操作要求极低。|依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。|
|模型定制|侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。|可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。|
|可解释性|可以追溯到具体的数据来源，有较好的可解释性和可追踪性。|黑盒子，可解释性相对较低。|
|计算资源|需要额外的资源来支持检索机制和数据库的维护。|依赖高质量的训练数据集和微调目标，对计算资源的要求较高。|
|推理延迟|增加了检索步骤的耗时|单纯 LLM 生成的耗时|
|降低幻觉|通过检索到的真实信息生成回答，降低了产生幻觉的概率。|模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。|
|伦理隐私|检索和使用外部数据可能引发伦理和隐私方面的问题。|训练数据中的敏感信息需要妥善处理，以防泄露。|

‍
